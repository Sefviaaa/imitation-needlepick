{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0350ba8",
   "metadata": {},
   "source": [
    "# BC LSTM Policy: Training, Evaluation, and Plotting Pipeline (100 Episodes)\n",
    "This notebook combines LSTM training, evaluation, and plotting using expert trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from surrol.tasks.needle_pick import NeedlePick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set random seeds for reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca180826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_obs(obs):\n",
    "    return np.concatenate([obs['observation'], obs['achieved_goal'], obs['desired_goal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516357b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Policy network (LSTM) ---\n",
    "class LSTMPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(obs_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, act_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch, seq_len, obs_dim)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        # Output action for every time step\n",
    "        out = self.fc(lstm_out)  # (batch, seq_len, act_dim)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare full-episode LSTM dataset from trajectories ---\n",
    "def make_lstm_episode_dataset(trajectories):\n",
    "    obs_episodes = []\n",
    "    act_episodes = []\n",
    "    for episode in trajectories:\n",
    "        obs_list = episode['observations']\n",
    "        act_list = episode['actions']\n",
    "        obs_arr = np.array([concat_obs(o) for o in obs_list])\n",
    "        act_arr = np.array(act_list)\n",
    "        obs_episodes.append(obs_arr)\n",
    "        act_episodes.append(act_arr)\n",
    "    return obs_episodes, act_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c683fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training function on full episodes ---\n",
    "def train_lstm_full_episodes(obs_episodes, act_episodes, epochs=50, lr=1e-3, hidden_size=128, num_layers=1, val_split=0.1):\n",
    "    obs_dim = obs_episodes[0].shape[1]\n",
    "    act_dim = act_episodes[0].shape[1]\n",
    "    policy = LSTMPolicy(obs_dim, act_dim, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Split for validation (by episode)\n",
    "    n_val = int(len(obs_episodes) * val_split)\n",
    "    if n_val > 0:\n",
    "        val_obs_episodes = obs_episodes[:n_val]\n",
    "        val_act_episodes = act_episodes[:n_val]\n",
    "        train_obs_episodes = obs_episodes[n_val:]\n",
    "        train_act_episodes = act_episodes[n_val:]\n",
    "    else:\n",
    "        train_obs_episodes = obs_episodes\n",
    "        train_act_episodes = act_episodes\n",
    "        val_obs_episodes = []\n",
    "        val_act_episodes = []\n",
    "\n",
    "    num_episodes = len(train_obs_episodes)\n",
    "    losses = []\n",
    "    val_mse = []\n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        epoch_loss = 0\n",
    "        total_steps = 0\n",
    "        indices = np.arange(num_episodes)\n",
    "        np.random.shuffle(indices)\n",
    "        for idx in indices:\n",
    "            obs_seq = torch.tensor(train_obs_episodes[idx], dtype=torch.float32).unsqueeze(0).to(device)  # (1, seq_len, obs_dim)\n",
    "            act_seq = torch.tensor(train_act_episodes[idx], dtype=torch.float32).unsqueeze(0).to(device)  # (1, seq_len, act_dim)\n",
    "            optimizer.zero_grad()\n",
    "            act_pred, _ = policy(obs_seq)\n",
    "            # Compute loss across all time steps in the episode\n",
    "            loss = loss_fn(act_pred, act_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * obs_seq.shape[1]\n",
    "            total_steps += obs_seq.shape[1]\n",
    "        avg_loss = epoch_loss / total_steps if total_steps > 0 else 0\n",
    "        losses.append(avg_loss)\n",
    "        # Validation\n",
    "        if val_obs_episodes:\n",
    "            policy.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_steps = 0\n",
    "                for obs_seq, act_seq in zip(val_obs_episodes, val_act_episodes):\n",
    "                    obs_seq_t = torch.tensor(obs_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    act_seq_t = torch.tensor(act_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    act_pred, _ = policy(obs_seq_t)\n",
    "                    l = loss_fn(act_pred, act_seq_t)\n",
    "                    val_loss += l.item() * obs_seq_t.shape[1]\n",
    "                    val_steps += obs_seq_t.shape[1]\n",
    "                val_mse.append(val_loss / val_steps if val_steps > 0 else 0)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == epochs:\n",
    "            if val_mse:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}, Val: {val_mse[-1]:.6f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    return policy, losses, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate LSTM policy step-by-step ---\n",
    "def evaluate_lstm_policy_sequential(policy, episodes=10, max_steps=200):\n",
    "    success_count = 0\n",
    "    returns = []\n",
    "    for ep in range(episodes):\n",
    "        env = NeedlePick(render_mode=None)\n",
    "        obs = env.reset()\n",
    "        obs_in = concat_obs(obs)\n",
    "        hidden = None\n",
    "        total_reward = 0\n",
    "        success = False\n",
    "        for t in range(max_steps):\n",
    "            inp = torch.tensor(obs_in, dtype=torch.float32).view(1, 1, -1).to(device)  # shape (1,1,obs_dim)\n",
    "            with torch.no_grad():\n",
    "                out, hidden = policy(inp, hidden)\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            action = out.cpu().numpy()[0, 0]\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            obs_in = concat_obs(obs)\n",
    "            total_reward += reward\n",
    "            if info.get('is_success', False):\n",
    "                success = True\n",
    "                break\n",
    "            if done:\n",
    "                break\n",
    "        returns.append(total_reward)\n",
    "        if success:\n",
    "            success_count += 1\n",
    "        env.close()\n",
    "    avg_return = np.mean(returns)\n",
    "    success_rate = success_count / episodes\n",
    "    return avg_return, success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb0272",
   "metadata": {},
   "source": [
    "# 1. Training: Hyperparameter Grid Search\n",
    "Train LSTM policies with different hyperparameters and save models/metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main pipeline ---\n",
    "out_dir = \"lstm_bc_models\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load expert data\n",
    "with open(\"expert_trajectories.pkl\", \"rb\") as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "# Prepare full-episode LSTM dataset\n",
    "obs_episodes, act_episodes = make_lstm_episode_dataset(trajectories)\n",
    "print(f\"Collected {len(obs_episodes)} LSTM episodes for training (full-episode).\")\n",
    "\n",
    "obs_dim = obs_episodes[0].shape[1]\n",
    "act_dim = act_episodes[0].shape[1]\n",
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-3, 3e-4, 1e-4]\n",
    "hidden_sizes = [128, 256]\n",
    "epochs_list = [75, 150]\n",
    "num_layers_list = [1, 2]\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Grid search (batch_size dropped since each episode is a batch)\n",
    "for lr in learning_rates:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for epochs in epochs_list:\n",
    "            for num_layer in num_layers_list:\n",
    "                print(\"\\n========================================\")\n",
    "                print(f\"Training LSTM (full-episode): lr={lr}, hidden={hidden_size}, epochs={epochs}, layers={num_layer}\")\n",
    "                policy, losses, val_mse = train_lstm_full_episodes(\n",
    "                    obs_episodes, act_episodes, epochs=epochs,\n",
    "                    lr=lr, hidden_size=hidden_size, num_layers=num_layer\n",
    "                )\n",
    "                # Save model and metrics\n",
    "                model_name = f\"lstm_bc_lr{lr}_hid{hidden_size}_ep{epochs}_lay{num_layer}_fullseq\"\n",
    "                torch.save(policy.state_dict(), os.path.join(out_dir, f\"{model_name}.pth\"))\n",
    "                np.save(os.path.join(out_dir, f\"{model_name}_train_losses.npy\"), np.array(losses))\n",
    "                np.save(os.path.join(out_dir, f\"{model_name}_val_mse.npy\"), np.array(val_mse))\n",
    "                print(f\"Saved model and logs to {out_dir}: {model_name}\")\n",
    "                # Store final val MSE for comparison\n",
    "                final_val = val_mse[-1] if val_mse else None\n",
    "                results.append({\n",
    "                    'lr': lr,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'epochs': epochs,\n",
    "                    'num_layers': num_layer,\n",
    "                    'final_val_mse': final_val,\n",
    "                    'model_name': model_name\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6ddf9",
   "metadata": {},
   "source": [
    "# 2. Evaluation: Policy Performance\n",
    "Evaluate each trained policy and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate all models ---\n",
    "eval_results = []\n",
    "EVAL_EPISODES = 10\n",
    "MAX_STEPS = 200\n",
    "for res in results:\n",
    "    model_name = res['model_name']\n",
    "    model_path = os.path.join(out_dir, f\"{model_name}.pth\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model {model_name} not found in {out_dir}. Skipping...\")\n",
    "        continue\n",
    "    print(f\"\\nEvaluating {model_name} (sequential LSTM)\")\n",
    "    policy = LSTMPolicy(obs_dim, act_dim, hidden_size=res['hidden_size'], num_layers=res['num_layers']).to(device)\n",
    "    policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    policy.eval()\n",
    "    avg_return, success_rate = evaluate_lstm_policy_sequential(\n",
    "        policy, episodes=EVAL_EPISODES, max_steps=MAX_STEPS)\n",
    "    print(f\"Result: Success rate: {success_rate*100:.1f}%, Avg return: {avg_return:.2f}\")\n",
    "    eval_results.append({\n",
    "        **res,\n",
    "        'avg_return': avg_return,\n",
    "        'success_rate': success_rate\n",
    "    })\n",
    "    np.save(os.path.join(out_dir, f\"{model_name}_eval_success_rate.npy\"), np.array([success_rate]))\n",
    "    np.save(os.path.join(out_dir, f\"{model_name}_eval_return.npy\"), np.array([avg_return]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fddd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save results to CSV ---\n",
    "csv_path = os.path.join(out_dir, \"evaluation_results.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"model_name\", \"lr\", \"hidden_size\", \"epochs\", \"num_layers\", \"final_val_mse\", \"avg_return\", \"success_rate\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print summary ---\n",
    "print(\"\\n===== LSTM Grid Evaluation Results (Sequential, Step-by-Step) =====\")\n",
    "for res in sorted(eval_results, key=lambda x: (-x['success_rate'], -x['avg_return'])):\n",
    "    print(f\"{res['model_name']}: Success={res['success_rate']*100:.1f}%, Return={res['avg_return']:.2f}\")\n",
    "print(f\"\\nAll models and metrics are saved in the folder: {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703161d8",
   "metadata": {},
   "source": [
    "# 3. Plotting: Visualize Training and Evaluation Metrics\n",
    "Plot training loss, validation MSE, success rate, and episode return for all hyperparameter configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42192724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plotting ----\n",
    "if not eval_results:\n",
    "    print(\"No results found, check that your metrics files exist and names match.\")\n",
    "else:\n",
    "    # Find the best configs for each metric\n",
    "    best_val = sorted(eval_results, key=lambda r: r['final_val_mse'])[0]\n",
    "    success_candidates = [r for r in eval_results if r['success_rate'] is not None]\n",
    "    if success_candidates:\n",
    "        best_success = sorted(success_candidates, key=lambda r: -r['success_rate'])[0]\n",
    "    else:\n",
    "        best_success = best_val\n",
    "    return_candidates = [r for r in eval_results if r['avg_return'] is not None]\n",
    "    if return_candidates:\n",
    "        best_return = sorted(return_candidates, key=lambda r: -r['avg_return'])[0]\n",
    "    else:\n",
    "        best_return = best_val\n",
    "\n",
    "    print(\"\\nBest by val MSE:\", best_val['model_name'])\n",
    "    print(\"Best by online success:\", best_success['model_name'])\n",
    "    print(\"Best by online return:\", best_return['model_name'])\n",
    "\n",
    "    models = [r['model_name'] for r in eval_results]\n",
    "    successes = [r['success_rate'] if r['success_rate'] is not None else 0.0 for r in eval_results]\n",
    "    returns = [r['avg_return'] if r['avg_return'] is not None else 0.0 for r in eval_results]\n",
    "\n",
    "    # 1. Training Loss (MSE) for best configs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.load(os.path.join(out_dir, f\"{best_val['model_name']}_train_losses.npy\")), label=f\"Best Val MSE ({best_val['model_name']})\")\n",
    "    plt.plot(np.load(os.path.join(out_dir, f\"{best_success['model_name']}_train_losses.npy\")), label=f\"Best Success ({best_success['model_name']})\")\n",
    "    plt.plot(np.load(os.path.join(out_dir, f\"{best_return['model_name']}_train_losses.npy\")), label=f\"Best Return ({best_return['model_name']})\")\n",
    "    plt.title(\"LSTM Training Loss (MSE) for Best Configs (100 Episodes)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"lstm_hyperparam_training_loss.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Validation MSE for best configs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.load(os.path.join(out_dir, f\"{best_val['model_name']}_val_mse.npy\")), label=f\"Best Val MSE ({best_val['model_name']})\")\n",
    "    plt.plot(np.load(os.path.join(out_dir, f\"{best_success['model_name']}_val_mse.npy\")), label=f\"Best Success ({best_success['model_name']})\")\n",
    "    plt.plot(np.load(os.path.join(out_dir, f\"{best_return['model_name']}_val_mse.npy\")), label=f\"Best Return ({best_return['model_name']})\")\n",
    "    plt.title(\"LSTM Validation MSE for Best Configs (100 Episodes)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend(fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"lstm_hyperparam_val_mse.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Task Success Rate (All Hyperparams)\n",
    "    success_sorted = sorted(zip(successes, models), reverse=True)\n",
    "    success_vals, success_labels = zip(*success_sorted)\n",
    "    plt.figure(figsize=(10, max(6, len(models)*0.3)))\n",
    "    plt.barh(range(len(success_vals)), success_vals, color='skyblue')\n",
    "    plt.yticks(range(len(success_labels)), success_labels, fontsize=7)\n",
    "    plt.xlabel(\"Success Rate\")\n",
    "    plt.title(\"LSTM Task Success Rate (All Hyperparams) (100 Episodes)\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"lstm_hyperparam_success_rate.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Episode Return (All Hyperparams)\n",
    "    return_sorted = sorted(zip(returns, models), reverse=True)\n",
    "    return_vals, return_labels = zip(*return_sorted)\n",
    "    plt.figure(figsize=(10, max(6, len(models)*0.3)))\n",
    "    plt.barh(range(len(return_vals)), return_vals, color='salmon')\n",
    "    plt.yticks(range(len(return_labels)), return_labels, fontsize=7)\n",
    "    plt.xlabel(\"Episode Return\")\n",
    "    plt.title(\"LSTM Episode Return (All Hyperparams)(100 Episodes)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"lstm_hyperparam_episode_return.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    print(\n",
    "        \"Plotted and saved in {}:\\n - Training loss: lstm_hyperparam_training_loss.png\\n - Validation MSE: lstm_hyperparam_val_mse.png\\n - Success rate: lstm_hyperparam_success_rate.png\\n - Episode return: lstm_hyperparam_episode_return.png\".format(out_dir)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
