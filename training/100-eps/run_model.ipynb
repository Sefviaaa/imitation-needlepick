{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ae6dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr 10 2025 14:42:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… surrol.gym/__init__.py has been imported!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from surrol.tasks.needle_pick import NeedlePick\n",
    "from best_seed_train import MLPPolicy, LSTMPolicy  # Adjust if your policy code is elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78cae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CONFIG ---\n",
    "model_path = \"best6_multiseed/lstm_best_seed3.pth\"   # Path to your .pth file\n",
    "model_type = \"lstm\"  # \"mlp\" or \"lstm\"\n",
    "render = True        # Set True to visualize, False for headless\n",
    "episode_seed = 3    # Use None for random, or set for reproducibility\n",
    "max_steps = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff82702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Observation/Action dims ---\n",
    "# Load one trajectory to get obs/act dims\n",
    "import pickle, os\n",
    "with open(os.path.join(\"expert_trajectories.pkl\"), \"rb\") as f:\n",
    "    expert_trajs = pickle.load(f)\n",
    "obs_example = expert_trajs[0]['observations'][0]\n",
    "obs_dim = (\n",
    "    obs_example['observation'].shape[0]\n",
    "    + obs_example['achieved_goal'].shape[0]\n",
    "    + obs_example['desired_goal'].shape[0]\n",
    ")\n",
    "act_dim = expert_trajs[0]['actions'][0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3570f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMPolicy(\n",
       "  (lstm): LSTM(25, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "# --- Build policy and load weights ---\n",
    "device = torch. device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Selected device:\", device)\n",
    "\n",
    "if model_type == \"mlp\":\n",
    "    policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=(256, 256)).to(device)\n",
    "elif model_type == \"lstm\":\n",
    "    policy = LSTMPolicy(obs_dim, act_dim, hidden_size=256, num_layers=2).to(device)\n",
    "else:\n",
    "    raise ValueError(\"model_type must be 'mlp' or 'lstm'\")\n",
    "\n",
    "policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "policy.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72176cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version = 4.1 Metal - 89.3\n",
      "Vendor = Apple\n",
      "Renderer = Apple M2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 11:51:06.600 python[83182:19864547] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-06-30 11:51:06.601 python[83182:19864547] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "Thread TERMINATED\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     27\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m---> 28\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_success\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Downloads/skripsi/pybullet/SurRoL/surrol/gym/surrol_env.py:100\u001b[0m, in \u001b[0;36mSurRoLEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_action(action)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# time1 = time.time()\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# TODO: check the best way to step simulation\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_duration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# time2 = time.time()\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# print(\" -> robot action time: {:.6f}, simulation time: {:.4f}\".format(time1 - time0, time2 - time1))\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_callback()\n",
      "File \u001b[0;32m~/Downloads/skripsi/pybullet/SurRoL/surrol/utils/pybullet_utils.py:20\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(duration)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m240\u001b[39m)):\n\u001b[0;32m---> 20\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Environment Setup ---\n",
    "if episode_seed is not None:\n",
    "    np.random.seed(episode_seed)\n",
    "env = NeedlePick(render_mode=\"human\" if render else None)\n",
    "obs = env.reset()\n",
    "\n",
    "def concat_obs(obs):\n",
    "    return np.concatenate([obs['observation'], obs['achieved_goal'], obs['desired_goal']])\n",
    "\n",
    "total_reward = 0\n",
    "success = False\n",
    "hidden = None\n",
    "\n",
    "for step in range(max_steps):\n",
    "    obs_in = concat_obs(obs)\n",
    "    if model_type == \"mlp\":\n",
    "        inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action = policy(inp).cpu().numpy().squeeze(0)\n",
    "    else:\n",
    "        inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_tensor, hidden = policy(inp, hidden)\n",
    "            action = action_tensor.cpu().numpy().squeeze(0).squeeze(0)\n",
    "    # Optional: clip to action space if needed\n",
    "    if hasattr(env, 'action_space'):\n",
    "        action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if info.get('is_success', False):\n",
    "        print(f\"Success at step {step}\")\n",
    "        success = True\n",
    "        break\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Episode finished. Total reward: {total_reward:.2f}, Success: {success}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surrol_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
