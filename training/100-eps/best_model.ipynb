{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872ab872",
   "metadata": {},
   "source": [
    "# Best Model: Training, Evaluation and Plotting (100 Episodes)\n",
    "This notebook combines training, evaluation, and plotting for all best-seed models across multiple seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from surrol.tasks.needle_pick import NeedlePick\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == CONFIG: Define your best hyperparameters for each model ==\n",
    "BEST_MODELS = [\n",
    "    {\n",
    "        \"name\": \"mlp_best\",\n",
    "        \"type\": \"mlp_bc\",\n",
    "        \"params\": dict(epochs=75, batch_size=64, lr=0.0003, hidden_sizes=(256,256))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lstm_best\",\n",
    "        \"type\": \"lstm_bc\",\n",
    "        \"params\": dict(epochs=150, lr=0.0003, hidden_size=256, num_layers=2)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mlp_dagger_best\",\n",
    "        \"type\": \"mlp_dagger\",\n",
    "        \"params\": dict(dagger_iters=5, epochs_dagger=30, batch_size=64, lr=0.001, hidden_sizes=(128,128))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mlp_dagger_tuned_best\",\n",
    "        \"type\": \"mlp_dagger_tuned\",\n",
    "        \"params\": dict(dagger_iters=5, epochs_dagger=30, batch_size=64, lr=0.0003, hidden_sizes=(256,256), expert_weight=0.8)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lstm_dagger_best\",\n",
    "        \"type\": \"lstm_dagger\",\n",
    "        \"params\": dict(dagger_iters=5, epochs_dagger=30, lr=0.001, hidden_size=128, num_layers=2)\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lstm_dagger_tuned_best\",\n",
    "        \"type\": \"lstm_dagger_tuned\",\n",
    "        \"params\": dict(dagger_iters=5, epochs_dagger=30, lr=0.001, hidden_size=128, num_layers=2, expert_weight=0.8)\n",
    "    }\n",
    "]\n",
    "NUM_SEEDS = 5\n",
    "SEED_LIST = [0, 1, 2, 3, 4]\n",
    "OUT_DIR = \"best6_multiseed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c991e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_obs(obs):\n",
    "    return np.concatenate([obs['observation'], obs['achieved_goal'], obs['desired_goal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(128,128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        input_dim = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        layers.append(nn.Linear(input_dim, act_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace323bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(obs_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, act_dim)\n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp_dataset(trajectories):\n",
    "    obs = []\n",
    "    acts = []\n",
    "    for episode in trajectories:\n",
    "        obs.extend([concat_obs(o) for o in episode['observations']])\n",
    "        acts.extend(episode['actions'])\n",
    "    return np.array(obs), np.array(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_episode_dataset(trajectories):\n",
    "    obs_episodes = []\n",
    "    act_episodes = []\n",
    "    for episode in trajectories:\n",
    "        obs_arr = np.array([concat_obs(o) for o in episode['observations']])\n",
    "        act_arr = np.array(episode['actions'])\n",
    "        obs_episodes.append(obs_arr)\n",
    "        act_episodes.append(act_arr)\n",
    "    return obs_episodes, act_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_bc(obs, acts, epochs=75, batch_size=128, lr=0.0001, hidden_sizes=(128, 128), val_split=0.1):\n",
    "    obs = np.array(obs)\n",
    "    acts = np.array(acts)\n",
    "    obs_dim = obs.shape[1]\n",
    "    act_dim = acts.shape[1]\n",
    "    policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=hidden_sizes).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    num_samples = len(obs)\n",
    "    idxs = np.arange(num_samples)\n",
    "    np.random.shuffle(idxs)\n",
    "    n_val = int(val_split * num_samples)\n",
    "    val_idx = idxs[:n_val]\n",
    "    train_idx = idxs[n_val:]\n",
    "    train_obs, train_acts = obs[train_idx], acts[train_idx]\n",
    "    val_obs, val_acts = obs[val_idx], acts[val_idx]\n",
    "    train_obs = torch.tensor(train_obs, dtype=torch.float32).to(device)\n",
    "    train_acts = torch.tensor(train_acts, dtype=torch.float32).to(device)\n",
    "    val_obs = torch.tensor(val_obs, dtype=torch.float32).to(device) if n_val > 0 else None\n",
    "    val_acts = torch.tensor(val_acts, dtype=torch.float32).to(device) if n_val > 0 else None\n",
    "    losses = []\n",
    "    val_mse = []\n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        epoch_loss = 0\n",
    "        idxs = np.random.permutation(train_obs.shape[0])\n",
    "        obs_shuffled = train_obs[idxs]\n",
    "        acts_shuffled = train_acts[idxs]\n",
    "        for i in range(0, len(obs_shuffled), batch_size):\n",
    "            obs_batch = obs_shuffled[i:i+batch_size]\n",
    "            act_batch = acts_shuffled[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            act_pred = policy(obs_batch)\n",
    "            loss = loss_fn(act_pred, act_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * obs_batch.shape[0]\n",
    "        avg_loss = epoch_loss / len(obs_shuffled)\n",
    "        losses.append(avg_loss)\n",
    "        # Validation\n",
    "        if val_obs is not None:\n",
    "            policy.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = policy(val_obs)\n",
    "                val_loss = loss_fn(val_pred, val_acts).item()\n",
    "                val_mse.append(val_loss)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == epochs:\n",
    "            if val_mse:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}, Val: {val_mse[-1]:.6f}\")\n",
    "            else:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    return policy, losses, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_weighted(expert_obs, expert_acts, dagger_obs, dagger_acts,\n",
    "                       expert_weight=0.8, epochs=150, batch_size=128, lr=0.001, hidden_sizes=(128,128), val_split=0.1):\n",
    "    obs = np.vstack([expert_obs, dagger_obs]) if len(dagger_obs) else expert_obs\n",
    "    acts = np.vstack([expert_acts, dagger_acts]) if len(dagger_acts) else expert_acts\n",
    "    weights = np.array([expert_weight]*len(expert_obs) + [1.0-expert_weight]*len(dagger_obs))\n",
    "    obs_dim = obs.shape[1]\n",
    "    act_dim = acts.shape[1]\n",
    "    policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=hidden_sizes).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    num_samples = len(obs)\n",
    "    idxs = np.arange(num_samples)\n",
    "    np.random.shuffle(idxs)\n",
    "    n_val = int(val_split * num_samples)\n",
    "    train_obs, train_acts, train_weights = obs[n_val:], acts[n_val:], weights[n_val:]\n",
    "    val_obs, val_acts, val_weights = obs[:n_val], acts[:n_val], weights[:n_val]\n",
    "    train_obs = torch.tensor(train_obs, dtype=torch.float32).to(device)\n",
    "    train_acts = torch.tensor(train_acts, dtype=torch.float32).to(device)\n",
    "    train_weights = torch.tensor(train_weights, dtype=torch.float32).to(device)\n",
    "    val_obs = torch.tensor(val_obs, dtype=torch.float32).to(device) if n_val > 0 else None\n",
    "    val_acts = torch.tensor(val_acts, dtype=torch.float32).to(device) if n_val > 0 else None\n",
    "    val_weights = torch.tensor(val_weights, dtype=torch.float32).to(device) if n_val > 0 else None\n",
    "    losses = []\n",
    "    val_mse = []\n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        epoch_loss = 0\n",
    "        idxs = np.random.permutation(train_obs.shape[0])\n",
    "        obs_shuffled = train_obs[idxs]\n",
    "        acts_shuffled = train_acts[idxs]\n",
    "        weights_shuffled = train_weights[idxs]\n",
    "        for i in range(0, len(obs_shuffled), batch_size):\n",
    "            obs_batch = obs_shuffled[i:i+batch_size]\n",
    "            act_batch = acts_shuffled[i:i+batch_size]\n",
    "            w_batch = weights_shuffled[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            act_pred = policy(obs_batch)\n",
    "            loss = loss_fn(act_pred, act_batch)\n",
    "            weighted_loss = (loss * w_batch.mean())\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += weighted_loss.item() * obs_batch.shape[0]\n",
    "        avg_loss = epoch_loss / len(obs_shuffled)\n",
    "        losses.append(avg_loss)\n",
    "        # Validation\n",
    "        if val_obs is not None:\n",
    "            policy.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = policy(val_obs)\n",
    "                val_loss = loss_fn(val_pred, val_acts).item()\n",
    "                val_mse.append(val_loss)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == epochs:\n",
    "            if val_mse:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Weighted Loss: {avg_loss:.6f}, Val: {val_mse[-1]:.6f}\")\n",
    "            else:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Weighted Loss: {avg_loss:.6f}\")\n",
    "    return policy, losses, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_bc(obs_episodes, act_episodes, epochs=150, lr=0.001, hidden_size=256, num_layers=2, val_split=0.1):\n",
    "    obs_dim = obs_episodes[0].shape[1]\n",
    "    act_dim = act_episodes[0].shape[1]\n",
    "    policy = LSTMPolicy(obs_dim, act_dim, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    n_val = int(len(obs_episodes) * val_split)\n",
    "    train_obs_episodes = obs_episodes[n_val:]\n",
    "    train_act_episodes = act_episodes[n_val:]\n",
    "    val_obs_episodes = obs_episodes[:n_val] if n_val > 0 else []\n",
    "    val_act_episodes = act_episodes[:n_val] if n_val > 0 else []\n",
    "    num_episodes = len(train_obs_episodes)\n",
    "    losses = []\n",
    "    val_mse = []\n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        epoch_loss = 0\n",
    "        total_steps = 0\n",
    "        idxs = np.random.permutation(num_episodes)\n",
    "        for epi_idx in idxs:\n",
    "            obs_seq = torch.tensor(train_obs_episodes[epi_idx], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            act_seq = torch.tensor(train_act_episodes[epi_idx], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            act_pred, _ = policy(obs_seq)\n",
    "            loss = loss_fn(act_pred, act_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * obs_seq.shape[1]\n",
    "            total_steps += obs_seq.shape[1]\n",
    "        avg_loss = epoch_loss / total_steps if total_steps > 0 else 0\n",
    "        losses.append(avg_loss)\n",
    "        # Validation\n",
    "        if val_obs_episodes:\n",
    "            policy.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_steps = 0\n",
    "                for obs_seq, act_seq in zip(val_obs_episodes, val_act_episodes):\n",
    "                    obs_seq_t = torch.tensor(obs_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    act_seq_t = torch.tensor(act_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    act_pred, _ = policy(obs_seq_t)\n",
    "                    l = loss_fn(act_pred, act_seq_t)\n",
    "                    val_loss += l.item() * obs_seq_t.shape[1]\n",
    "                    val_steps += obs_seq_t.shape[1]\n",
    "                val_mse.append(val_loss / val_steps if val_steps > 0 else 0)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == epochs:\n",
    "            if val_mse:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}, Val: {val_mse[-1]:.6f}\")\n",
    "            else:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "    return policy, losses, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_weighted(expert_obs_episodes, expert_act_episodes, dagger_obs_episodes, dagger_act_episodes,\n",
    "                        expert_weight=0.8, epochs=150, lr=0.001, hidden_size=128, num_layers=1, val_split=0.1):\n",
    "    obs_dim = expert_obs_episodes[0].shape[1]\n",
    "    act_dim = expert_act_episodes[0].shape[1]\n",
    "    policy = LSTMPolicy(obs_dim, act_dim, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    obs_episodes = list(expert_obs_episodes) + list(dagger_obs_episodes)\n",
    "    act_episodes = list(expert_act_episodes) + list(dagger_act_episodes)\n",
    "    weights = np.array([expert_weight]*len(expert_obs_episodes) + [1.0-expert_weight]*len(dagger_obs_episodes))\n",
    "    indices = np.arange(len(obs_episodes))\n",
    "    np.random.shuffle(indices)\n",
    "    obs_episodes = [obs_episodes[i] for i in indices]\n",
    "    act_episodes = [act_episodes[i] for i in indices]\n",
    "    weights = weights[indices]\n",
    "    n_val = int(len(obs_episodes) * val_split)\n",
    "    train_obs_episodes = obs_episodes[n_val:]\n",
    "    train_act_episodes = act_episodes[n_val:]\n",
    "    train_weights = weights[n_val:]\n",
    "    num_episodes = len(train_obs_episodes)\n",
    "    losses = []\n",
    "    val_mse = []\n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        epoch_loss = 0\n",
    "        total_steps = 0\n",
    "        indices = np.arange(num_episodes)\n",
    "        np.random.shuffle(indices)\n",
    "        for epi_idx in indices:\n",
    "            obs_seq = torch.tensor(train_obs_episodes[epi_idx], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            act_seq = torch.tensor(train_act_episodes[epi_idx], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            act_pred, _ = policy(obs_seq)\n",
    "            loss = loss_fn(act_pred, act_seq)\n",
    "            weighted_loss = loss * train_weights[epi_idx]\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += weighted_loss.item() * obs_seq.shape[1]\n",
    "            total_steps += obs_seq.shape[1]\n",
    "        avg_loss = epoch_loss / total_steps if total_steps > 0 else 0\n",
    "        losses.append(avg_loss)\n",
    "        # Validation\n",
    "        if n_val > 0:\n",
    "            policy.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                val_steps = 0\n",
    "                val_obs_episodes = obs_episodes[:n_val]\n",
    "                val_act_episodes = act_episodes[:n_val]\n",
    "                val_weights = weights[:n_val]\n",
    "                for obs_seq, act_seq, w in zip(val_obs_episodes, val_act_episodes, val_weights):\n",
    "                    obs_seq_t = torch.tensor(obs_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    act_seq_t = torch.tensor(act_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                    act_pred, _ = policy(obs_seq_t)\n",
    "                    l = loss_fn(act_pred, act_seq_t) * w\n",
    "                    val_loss += l.item() * obs_seq_t.shape[1]\n",
    "                    val_steps += obs_seq_t.shape[1]\n",
    "                val_mse.append(val_loss / val_steps if val_steps > 0 else 0)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == epochs:\n",
    "            if val_mse:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Weighted Loss: {avg_loss:.6f}, Val: {val_mse[-1]:.6f}\")\n",
    "            else:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs}, Weighted Loss: {avg_loss:.6f}\")\n",
    "    return policy, losses, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4900a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dagger_mlp_episodes_filtered(policy, env, num_episodes=5, max_steps=200):\n",
    "    dagger_obs = []\n",
    "    dagger_acts = []\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        obs_seq = []\n",
    "        act_seq = []\n",
    "        success = False\n",
    "        for t in range(max_steps):\n",
    "            obs_in = concat_obs(obs)\n",
    "            obs_seq.append(obs_in)\n",
    "            inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                action = policy(inp).cpu().numpy().squeeze(0)\n",
    "            expert_act = env.get_oracle_action(obs)\n",
    "            act_seq.append(expert_act)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if info.get(\"is_success\", False):\n",
    "                success = True\n",
    "                break\n",
    "            if done:\n",
    "                break\n",
    "        if success:\n",
    "            dagger_obs.extend(obs_seq)\n",
    "            dagger_acts.extend(act_seq)\n",
    "    return np.array(dagger_obs), np.array(dagger_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dagger_lstm_episodes_filtered(policy, env, num_episodes=5, max_steps=200):\n",
    "    dagger_obs_episodes = []\n",
    "    dagger_act_episodes = []\n",
    "    for ep in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        obs_seq = []\n",
    "        act_seq = []\n",
    "        success = False\n",
    "        for t in range(max_steps):\n",
    "            obs_in = concat_obs(obs)\n",
    "            obs_seq.append(obs_in)\n",
    "            inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                action, _ = policy(inp)\n",
    "                action = action.cpu().numpy().squeeze(0).squeeze(0)\n",
    "            expert_act = env.get_oracle_action(obs)\n",
    "            act_seq.append(expert_act)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if info.get(\"is_success\", False):\n",
    "                success = True\n",
    "                break\n",
    "            if done:\n",
    "                break\n",
    "        if success:\n",
    "            dagger_obs_episodes.append(np.array(obs_seq))\n",
    "            dagger_act_episodes.append(np.array(act_seq))\n",
    "    return dagger_obs_episodes, dagger_act_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch. device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762074b5",
   "metadata": {},
   "source": [
    "## Training: Train all models for all seeds and save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba36f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('mps' if torch.cuda.is_available() else \"cpu\")\n",
    "    with open(\"expert_trajectories.pkl\", \"rb\") as f:\n",
    "        expert_trajectories = pickle.load(f)\n",
    "    obs, acts = make_mlp_dataset(expert_trajectories)\n",
    "    obs_dim = obs.shape[1]\n",
    "    act_dim = acts.shape[1]\n",
    "    obs_episodes, act_episodes = make_lstm_episode_dataset(expert_trajectories)\n",
    "\n",
    "    for model_cfg in BEST_MODELS:\n",
    "        print(f\"\\n========== Training model: {model_cfg['name']} ==========\")\n",
    "        for seed in SEED_LIST:\n",
    "            print(f\"[{model_cfg['name']}] Seed {seed} ...\")\n",
    "            set_all_seeds(seed)\n",
    "            model_base = f\"{model_cfg['name']}_seed{seed}\"\n",
    "            train_losses, val_mse = None, None\n",
    "\n",
    "            if model_cfg['type'] == \"mlp_bc\":\n",
    "                policy, train_losses, val_mse = train_mlp_bc(\n",
    "                    obs, acts,\n",
    "                    epochs=model_cfg['params']['epochs'],\n",
    "                    batch_size=model_cfg['params']['batch_size'],\n",
    "                    lr=model_cfg['params']['lr'],\n",
    "                    hidden_sizes=model_cfg['params']['hidden_sizes']\n",
    "                )\n",
    "            elif model_cfg['type'] == \"lstm_bc\":\n",
    "                policy, train_losses, val_mse = train_lstm_bc(\n",
    "                    obs_episodes, act_episodes,\n",
    "                    epochs=model_cfg['params']['epochs'],\n",
    "                    lr=model_cfg['params']['lr'],\n",
    "                    hidden_size=model_cfg['params']['hidden_size'],\n",
    "                    num_layers=model_cfg['params']['num_layers']\n",
    "                )\n",
    "            elif model_cfg['type'] == \"mlp_dagger\":\n",
    "                dagger_obs = np.empty((0, obs.shape[1]), dtype=np.float32)\n",
    "                dagger_acts = np.empty((0, acts.shape[1]), dtype=np.float32)\n",
    "                env = NeedlePick(render_mode=None)\n",
    "                mlp_policy = None\n",
    "                all_train_losses, all_val_mse = [], []\n",
    "                for i in range(model_cfg['params']['dagger_iters']):\n",
    "                    print(f\"  [MLP DAgger] Iter {i+1}/{model_cfg['params']['dagger_iters']}\")\n",
    "                    mlp_policy, train_losses, val_mse = train_mlp_bc(\n",
    "                        np.vstack([obs, dagger_obs]) if dagger_obs.shape[0] else obs,\n",
    "                        np.vstack([acts, dagger_acts]) if dagger_acts.shape[0] else acts,\n",
    "                        epochs=model_cfg['params']['epochs_dagger'],\n",
    "                        batch_size=model_cfg['params']['batch_size'],\n",
    "                        lr=model_cfg['params']['lr'],\n",
    "                        hidden_sizes=model_cfg['params']['hidden_sizes'])\n",
    "                    if train_losses is not None:\n",
    "                        all_train_losses += list(train_losses)\n",
    "                    if val_mse is not None:\n",
    "                        all_val_mse += list(val_mse)\n",
    "                    new_obs, new_acts = collect_dagger_mlp_episodes_filtered(\n",
    "                        mlp_policy, env, num_episodes=10, max_steps=200)\n",
    "                    if new_obs.shape[0] > 0:\n",
    "                        dagger_obs = np.vstack([dagger_obs, new_obs])\n",
    "                        dagger_acts = np.vstack([dagger_acts, new_acts])\n",
    "                    print(f\"    DAgger: Collected {new_obs.shape[0]} new episodes. Aggregated dataset: {obs.shape[0] + dagger_obs.shape[0]} samples\")\n",
    "                policy = mlp_policy\n",
    "                train_losses = all_train_losses\n",
    "                val_mse = all_val_mse\n",
    "                del env\n",
    "            elif model_cfg['type'] == \"mlp_dagger_tuned\":\n",
    "                dagger_obs = np.empty((0, obs.shape[1]), dtype=np.float32)\n",
    "                dagger_acts = np.empty((0, acts.shape[1]), dtype=np.float32)\n",
    "                env = NeedlePick(render_mode=None)\n",
    "                mlp_policy = None\n",
    "                all_train_losses, all_val_mse = [], []\n",
    "                for i in range(model_cfg['params']['dagger_iters']):\n",
    "                    print(f\"  [MLP DAgger Tuned] Iter {i+1}/{model_cfg['params']['dagger_iters']}\")\n",
    "                    mlp_policy, train_losses, val_mse = train_mlp_weighted(\n",
    "                        obs, acts, dagger_obs, dagger_acts,\n",
    "                        expert_weight=model_cfg['params']['expert_weight'],\n",
    "                        epochs=model_cfg['params']['epochs_dagger'],\n",
    "                        batch_size=model_cfg['params']['batch_size'],\n",
    "                        lr=model_cfg['params']['lr'],\n",
    "                        hidden_sizes=model_cfg['params']['hidden_sizes'])\n",
    "                    if train_losses is not None:\n",
    "                        all_train_losses += list(train_losses)\n",
    "                    if val_mse is not None:\n",
    "                        all_val_mse += list(val_mse)\n",
    "                    new_obs, new_acts = collect_dagger_mlp_episodes_filtered(\n",
    "                        mlp_policy, env, num_episodes=10, max_steps=200)\n",
    "                    if new_obs.shape[0] > 0:\n",
    "                        dagger_obs = np.vstack([dagger_obs, new_obs])\n",
    "                        dagger_acts = np.vstack([dagger_acts, new_acts])\n",
    "                    print(f\"    DAgger: Collected {new_obs.shape[0]} new episodes. Aggregated dataset: {obs.shape[0] + dagger_obs.shape[0]} samples\")\n",
    "                policy = mlp_policy\n",
    "                train_losses = all_train_losses\n",
    "                val_mse = all_val_mse\n",
    "                del env\n",
    "            elif model_cfg['type'] == \"lstm_dagger\":\n",
    "                expert_obs_episodes, expert_act_episodes = make_lstm_episode_dataset(expert_trajectories)\n",
    "                dagger_obs_episodes = []\n",
    "                dagger_act_episodes = []\n",
    "                env = NeedlePick(render_mode=None)\n",
    "                lstm_policy = None\n",
    "                all_train_losses, all_val_mse = [], []\n",
    "                for i in range(model_cfg['params']['dagger_iters']):\n",
    "                    print(f\"  [LSTM DAgger] Iter {i+1}/{model_cfg['params']['dagger_iters']}\")\n",
    "                    lstm_policy, train_losses, val_mse = train_lstm_bc(\n",
    "                        expert_obs_episodes + dagger_obs_episodes,\n",
    "                        expert_act_episodes + dagger_act_episodes,\n",
    "                        epochs=model_cfg['params']['epochs_dagger'],\n",
    "                        lr=model_cfg['params']['lr'],\n",
    "                        hidden_size=model_cfg['params']['hidden_size'],\n",
    "                        num_layers=model_cfg['params']['num_layers'])\n",
    "                    if train_losses is not None:\n",
    "                        all_train_losses += list(train_losses)\n",
    "                    if val_mse is not None:\n",
    "                        all_val_mse += list(val_mse)\n",
    "                    new_obs, new_acts = collect_dagger_lstm_episodes_filtered(\n",
    "                        lstm_policy, env, num_episodes=10, max_steps=200)\n",
    "                    dagger_obs_episodes += new_obs\n",
    "                    dagger_act_episodes += new_acts\n",
    "                    print(f\"    DAgger: Collected {len(new_obs)} new episodes. Aggregated dataset: {len(expert_obs_episodes)+len(dagger_obs_episodes)} episodes\")\n",
    "                policy = lstm_policy\n",
    "                train_losses = all_train_losses\n",
    "                val_mse = all_val_mse\n",
    "                del env\n",
    "            elif model_cfg['type'] == \"lstm_dagger_tuned\":\n",
    "                expert_obs_episodes, expert_act_episodes = make_lstm_episode_dataset(expert_trajectories)\n",
    "                dagger_obs_episodes = []\n",
    "                dagger_act_episodes = []\n",
    "                env = NeedlePick(render_mode=None)\n",
    "                lstm_policy = None\n",
    "                all_train_losses, all_val_mse = [], []\n",
    "                for i in range(model_cfg['params']['dagger_iters']):\n",
    "                    print(f\"  [LSTM DAgger Tuned] Iter {i+1}/{model_cfg['params']['dagger_iters']}\")\n",
    "                    lstm_policy, train_losses, val_mse = train_lstm_weighted(\n",
    "                        expert_obs_episodes, expert_act_episodes,\n",
    "                        dagger_obs_episodes, dagger_act_episodes,\n",
    "                        expert_weight=model_cfg['params']['expert_weight'],\n",
    "                        epochs=model_cfg['params']['epochs_dagger'],\n",
    "                        lr=model_cfg['params']['lr'],\n",
    "                        hidden_size=model_cfg['params']['hidden_size'],\n",
    "                        num_layers=model_cfg['params']['num_layers'])\n",
    "                    if train_losses is not None:\n",
    "                        all_train_losses += list(train_losses)\n",
    "                    if val_mse is not None:\n",
    "                        all_val_mse += list(val_mse)\n",
    "                    new_obs, new_acts = collect_dagger_lstm_episodes_filtered(\n",
    "                        lstm_policy, env, num_episodes=10, max_steps=200)\n",
    "                    dagger_obs_episodes += new_obs\n",
    "                    dagger_act_episodes += new_acts\n",
    "                    print(f\"    DAgger: Collected {len(new_obs)} new episodes. Aggregated dataset: {len(expert_obs_episodes)+len(dagger_obs_episodes)} episodes\")\n",
    "                policy = lstm_policy\n",
    "                train_losses = all_train_losses\n",
    "                val_mse = all_val_mse\n",
    "                del env\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type {model_cfg['type']}\")\n",
    "\n",
    "            # ---- SAVE LOSSES & VAL ----\n",
    "            if train_losses is not None:\n",
    "                np.save(os.path.join(OUT_DIR, f\"{model_base}_train_losses.npy\"), np.array(train_losses))\n",
    "            if val_mse is not None:\n",
    "                np.save(os.path.join(OUT_DIR, f\"{model_base}_val_mse.npy\"), np.array(val_mse))\n",
    "\n",
    "            torch.save(policy.state_dict(), os.path.join(OUT_DIR, f\"{model_base}.pth\"))\n",
    "        print(f\"\\nAll models for {model_cfg['name']} trained and saved in {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b5836",
   "metadata": {},
   "source": [
    "## Evaluation: Evaluate all models for all seeds and save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f75415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation setup for all models and all seeds ---\n",
    "results = []\n",
    "with open(\"expert_trajectories.pkl\", \"rb\") as f:\n",
    "    trajectories = pickle.load(f)\n",
    "obs_example = trajectories[0]['observations'][0]\n",
    "obs_dim = (\n",
    "    obs_example['observation'].shape[0] +\n",
    "    obs_example['achieved_goal'].shape[0] +\n",
    "    obs_example['desired_goal'].shape[0]\n",
    ")\n",
    "act_dim = trajectories[0]['actions'][0].shape[0]\n",
    "\n",
    "def evaluate_policy(policy, model_type, episodes=10, max_steps=200, save_traj_path=None):\n",
    "    env = NeedlePick(render_mode=None)\n",
    "    success_count = 0\n",
    "    returns = []\n",
    "    all_trajectories = []\n",
    "    for ep in range(episodes):\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        hidden = None\n",
    "        traj = {'obs': [], 'actions': [], 'rewards': [], 'infos': []}\n",
    "        for step in range(max_steps):\n",
    "            obs_in = concat_obs(obs)\n",
    "            if model_type.startswith(\"mlp\"):\n",
    "                inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action = policy(inp).cpu().numpy().squeeze(0)\n",
    "            else:\n",
    "                inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    action_tensor, hidden = policy(inp, hidden)\n",
    "                    action = action_tensor.cpu().numpy().squeeze(0).squeeze(0)\n",
    "            if hasattr(env, 'action_space'):\n",
    "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            obs_next, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            traj['obs'].append(obs)\n",
    "            traj['actions'].append(action)\n",
    "            traj['rewards'].append(reward)\n",
    "            traj['infos'].append(info)\n",
    "            obs = obs_next\n",
    "            if info.get('is_success', False):\n",
    "                success_count += 1\n",
    "                break\n",
    "            if done:\n",
    "                break\n",
    "        returns.append(total_reward)\n",
    "        all_trajectories.append(traj)\n",
    "    if save_traj_path is not None:\n",
    "        with open(save_traj_path, \"wb\") as f:\n",
    "            pickle.dump(all_trajectories, f)\n",
    "    avg_return = np.mean(returns)\n",
    "    success_rate = success_count / episodes\n",
    "    try:\n",
    "        env.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Exception during env.close(): {e}\")\n",
    "    del env\n",
    "    return avg_return, success_rate\n",
    "\n",
    "for model_cfg in BEST_MODELS:\n",
    "    for seed in SEED_LIST:\n",
    "        model_name = f\"{model_cfg['name']}_seed{seed}\"\n",
    "        model_path = os.path.join(OUT_DIR, f\"{model_name}.pth\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model {model_name} not found in {OUT_DIR}. Skipping...\")\n",
    "            continue\n",
    "        print(f\"\\nEvaluating {model_name}\")\n",
    "        if model_cfg[\"type\"].startswith(\"mlp\"):\n",
    "            policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=model_cfg[\"params\"][\"hidden_sizes\"]).to(device)\n",
    "        else:\n",
    "            hidden_size = model_cfg[\"params\"].get(\"hidden_size\", 128)\n",
    "            num_layers = model_cfg[\"params\"].get(\"num_layers\", 1)\n",
    "            policy = LSTMPolicy(obs_dim, act_dim, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "        policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        policy.eval()\n",
    "        traj_save_path = os.path.join(OUT_DIR, f\"{model_name}_eval_traj.pkl\")\n",
    "        avg_return, success_rate = evaluate_policy(\n",
    "            policy, model_cfg[\"type\"], episodes=10, max_steps=200, save_traj_path=traj_save_path)\n",
    "        print(f\"Result: Success rate: {success_rate*100:.1f}%, Avg return: {avg_return:.2f}\")\n",
    "        results.append({\n",
    "            'model_name': model_name,\n",
    "            'model_type': model_cfg[\"type\"],\n",
    "            'seed': seed,\n",
    "            'avg_return': avg_return,\n",
    "            'success_rate': success_rate,\n",
    "            **model_cfg[\"params\"],\n",
    "        })\n",
    "        np.save(os.path.join(OUT_DIR, f\"{model_name}_eval_success_rate.npy\"), np.array([success_rate]))\n",
    "        np.save(os.path.join(OUT_DIR, f\"{model_name}_eval_return.npy\"), np.array([avg_return]))\n",
    "\n",
    "# --- Save results to CSV ---\n",
    "csv_path = os.path.join(OUT_DIR, \"evaluation_results_best6_multiseed.csv\")\n",
    "columns = [\"model_name\", \"model_type\", \"seed\",\n",
    "           \"avg_return\", \"success_rate\",\n",
    "           \"epochs\", \"batch_size\", \"lr\", \"hidden_sizes\", \"hidden_size\", \"num_layers\",\n",
    "           \"dagger_iters\", \"epochs_dagger\", \"expert_weight\"]\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "# --- Print summary across seeds for each model ---\n",
    "agg = defaultdict(list)\n",
    "for res in results:\n",
    "    model_base = res['model_name'].rsplit(\"_seed\", 1)[0]\n",
    "    agg[model_base].append(res)\n",
    "\n",
    "print(\"\\n===== BEST6 Multiseed Evaluation Results (ALL SEEDS) =====\")\n",
    "for model_base, group in agg.items():\n",
    "    sr = np.array([r['success_rate'] for r in group])\n",
    "    ret = np.array([r['avg_return'] for r in group])\n",
    "    print(f\"{model_base}:\")\n",
    "    print(f\"    Success Rate: {sr.mean()*100:.2f}% ± {sr.std()*100:.2f}%\")\n",
    "    print(f\"    Avg Return: {ret.mean():.2f} ± {ret.std():.2f}\")\n",
    "\n",
    "print(\"\\n===== All Individual Evaluation Results =====\")\n",
    "for res in sorted(results, key=lambda x: (-x['success_rate'], -x['avg_return'])):\n",
    "    print(f\"{res['model_name']}: Success={res['success_rate']*100:.1f}%, Return={res['avg_return']:.2f}\")\n",
    "\n",
    "print(f\"\\nSaved evaluation results and metrics to folder: {OUT_DIR}\")\n",
    "print(\"Trajectory for each evaluation is saved as *_eval_traj.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1c6a5",
   "metadata": {},
   "source": [
    "## Plotting: Visualize results across all models and seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Directory where models/metrics are saved ----\n",
    "out_dir = OUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model configs (must match your training/eval code) ----\n",
    "SEED_LIST = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Collect results ----\n",
    "results = []\n",
    "for model_cfg in BEST_MODELS:\n",
    "    for seed in SEED_LIST:\n",
    "        model_name = f\"{model_cfg['name']}_seed{seed}\"\n",
    "        res = {'model_name': model_name, 'model_type': model_cfg['type'], 'seed': seed, **model_cfg['params']}\n",
    "        # Try to find and load metrics if available for plotting (train/val loss may not be saved for all)\n",
    "        for metric in [\"train_losses\", \"val_mse\", \"eval_success_rate\", \"eval_return\"]:\n",
    "            metric_path = os.path.join(out_dir, f\"{model_name}_{metric}.npy\")\n",
    "            if os.path.exists(metric_path):\n",
    "                res[metric] = np.load(metric_path)\n",
    "            else:\n",
    "                res[metric] = None\n",
    "        results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb89ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results:\n",
    "    print(\"No results found, check that your metrics files exist and names match.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4da4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Group by model type for summary stats ----\n",
    "grouped = defaultdict(list)\n",
    "for r in results:\n",
    "    grouped[r['model_name'].rsplit(\"_seed\", 1)[0]].append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee62d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Compute mean/std for each model config ----\n",
    "summary = []\n",
    "for model_base, group in grouped.items():\n",
    "    n = len(group)\n",
    "    success_rates = [float(g['eval_success_rate'][0]) for g in group if g['eval_success_rate'] is not None]\n",
    "    returns = [float(g['eval_return'][0]) for g in group if g['eval_return'] is not None]\n",
    "    # For training/val loss, average curves if available\n",
    "    train_curves = [g['train_losses'] for g in group if g['train_losses'] is not None]\n",
    "    val_curves = [g['val_mse'] for g in group if g['val_mse'] is not None]\n",
    "    summary.append({\n",
    "        'model_base': model_base,\n",
    "        'model_type': group[0]['model_type'],\n",
    "        'success_rate_mean': np.mean(success_rates) if success_rates else None,\n",
    "        'success_rate_std': np.std(success_rates) if success_rates else None,\n",
    "        'return_mean': np.mean(returns) if returns else None,\n",
    "        'return_std': np.std(returns) if returns else None,\n",
    "        'success_rates': success_rates,\n",
    "        'returns': returns,\n",
    "        'train_curves': train_curves,\n",
    "        'val_curves': val_curves\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Bar plot: Success Rate for all model configs (mean ± std) ----\n",
    "plt.figure(figsize=(10, max(6, len(summary)*0.5)))\n",
    "sorted_summary = sorted(summary, key=lambda s: -(s['success_rate_mean'] if s['success_rate_mean'] is not None else -1))\n",
    "labels = [s['model_base'] for s in sorted_summary]\n",
    "means = [s['success_rate_mean'] for s in sorted_summary]\n",
    "stds = [s['success_rate_std'] for s in sorted_summary]\n",
    "plt.barh(range(len(means)), means, xerr=stds, color='skyblue')\n",
    "plt.yticks(range(len(labels)), labels, fontsize=9)\n",
    "plt.xlabel(\"Success Rate (mean ± std, N=5 seeds)\")\n",
    "plt.title(\"Task Success Rate (100 Demonstration Episodes)\")\n",
    "plt.xlim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"best6_multiseed_success_rate.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1b. Also plot each individual seed as points on the bar plot for Success Rate ----\n",
    "plt.figure(figsize=(10, max(6, len(summary)*0.5)))\n",
    "plt.barh(range(len(means)), means, xerr=stds, color='skyblue', alpha=0.7, label=\"Mean ± std\")\n",
    "for i, s in enumerate(sorted_summary):\n",
    "    plt.scatter([val for val in s['success_rates']], [i]*len(s['success_rates']), color='k', marker='|', s=100, label=\"Seed values\" if i==0 else \"\")\n",
    "plt.yticks(range(len(labels)), labels, fontsize=9)\n",
    "plt.xlabel(\"Success Rate\")\n",
    "plt.title(\"Task Success Rate (100 Demonstration Episodes)\")\n",
    "plt.xlim([0, 1])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"best6_multiseed_success_rate_with_seeds.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2. Bar plot: Return for all model configs (mean ± std) ----\n",
    "plt.figure(figsize=(10, max(6, len(summary)*0.5)))\n",
    "means = [s['return_mean'] for s in sorted_summary]\n",
    "stds = [s['return_std'] for s in sorted_summary]\n",
    "plt.barh(range(len(means)), means, xerr=stds, color='salmon')\n",
    "plt.yticks(range(len(labels)), labels, fontsize=9)\n",
    "plt.xlabel(\"Episode Return (mean ± std, N=5 seeds)\")\n",
    "plt.title(\"Episode Return (100 Demonstration Episodes)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"best6_multiseed_episode_return.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c883e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2b. Also plot each individual seed as points on the bar plot for Return ----\n",
    "plt.figure(figsize=(10, max(6, len(summary)*0.5)))\n",
    "plt.barh(range(len(means)), means, xerr=stds, color='salmon', alpha=0.7, label=\"Mean ± std\")\n",
    "for i, s in enumerate(sorted_summary):\n",
    "    plt.scatter([val for val in s['returns']], [i]*len(s['returns']), color='k', marker='|', s=100, label=\"Seed values\" if i==0 else \"\")\n",
    "plt.yticks(range(len(labels)), labels, fontsize=9)\n",
    "plt.xlabel(\"Episode Return\")\n",
    "plt.title(\"Episode Return (100 Demonstration Episodes)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"best6_multiseed_episode_return_with_seeds.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfe532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Plotted and saved in {out_dir}:\\n\"\n",
    "    \" - Success rate: best6_multiseed_success_rate.png\\n\"\n",
    "    \" - Success rate with all seeds: best6_multiseed_success_rate_with_seeds.png\\n\"\n",
    "    \" - Episode return: best6_multiseed_episode_return.png\\n\"\n",
    "    \" - Episode return with all seeds: best6_multiseed_episode_return_with_seeds.png\\n\"\n",
    "    \" - Training loss (best model): *_training_loss_seeds.png\\n\"\n",
    "    \" - Validation MSE (best model): *_val_mse_seeds.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_model_name(model_base):\n",
    "    name = model_base.replace(\"_\", \" \").replace(\"dagger\", \"Dagger\").replace(\"mlp\", \"MLP\").replace(\"lstm\", \"LSTM\")\n",
    "    name = name.replace(\"tuned\", \"Tuned\")\n",
    "    if name.lower().endswith(\" best\"):\n",
    "        name = name[:-5]\n",
    "    return \" \".join(w.capitalize() if not w.isupper() else w for w in name.split())\n",
    "\n",
    "for s in sorted_summary:\n",
    "    model_label = pretty_model_name(s['model_base'])\n",
    "    if s['train_curves']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, curve in enumerate(s['train_curves']):\n",
    "            plt.plot(curve, alpha=0.5, label=f\"Seed {i}\")\n",
    "        if len(s['train_curves']) > 1:\n",
    "            train_mean = np.mean(np.array(s['train_curves']), axis=0)\n",
    "            train_std = np.std(np.array(s['train_curves']), axis=0)\n",
    "            plt.plot(train_mean, color='k', label=\"Mean\", linewidth=2)\n",
    "            plt.fill_between(range(len(train_mean)), train_mean-train_std, train_mean+train_std, color='k', alpha=0.2, label=\"Mean ± std\")\n",
    "        plt.title(f\"Training Loss (MSE) for {model_label} (100 Demonstration Episodes)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend(fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"{s['model_base']}_training_loss_seeds.png\"))\n",
    "        plt.show()\n",
    "    if s['val_curves']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, curve in enumerate(s['val_curves']):\n",
    "            plt.plot(curve, alpha=0.5, label=f\"Seed {i}\")\n",
    "        if len(s['val_curves']) > 1:\n",
    "            val_mean = np.mean(np.array(s['val_curves']), axis=0)\n",
    "            val_std = np.std(np.array(s['val_curves']), axis=0)\n",
    "            plt.plot(val_mean, color='k', label=\"Mean\", linewidth=2)\n",
    "            plt.fill_between(range(len(val_mean)), val_mean-val_std, val_mean+val_std, color='k', alpha=0.2, label=\"Mean ± std\")\n",
    "        plt.title(f\"Validation MSE for {model_label} (100 Demonstration Episodes)\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.legend(fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"{s['model_base']}_val_mse_seeds.png\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51628e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_LIST = [0, 1, 2, 3, 4]\n",
    "seed_colors = [\n",
    "    \"#7eb6f6\",   # Seed 0 - biru muda\n",
    "    \"#f3c37c\",   # Seed 1 - oranye muda\n",
    "    \"#a6d9a7\",   # Seed 2 - hijau muda\n",
    "    \"#ea9498\",   # Seed 3 - merah muda\n",
    "    \"#b2abd2\",   # Seed 4 - ungu muda\n",
    "]\n",
    "labels = [s['model_base'] for s in sorted_summary]\n",
    "means = [s['success_rate_mean'] for s in sorted_summary]\n",
    "stds = [s['success_rate_std'] for s in sorted_summary]\n",
    "plt.figure(figsize=(10, max(6, len(labels)*0.5)))\n",
    "plt.barh(range(len(means)), means, xerr=stds, color='skyblue', alpha=0.7, label=\"Mean ± std\")\n",
    "for i, s in enumerate(sorted_summary):\n",
    "    val_to_seeds = {}\n",
    "    val_to_colors = {}\n",
    "    for j, val in enumerate(s['success_rates']):\n",
    "        if val not in val_to_seeds:\n",
    "            val_to_seeds[val] = []\n",
    "            val_to_colors[val] = []\n",
    "        val_to_seeds[val].append(SEED_LIST[j])\n",
    "        val_to_colors[val].append(seed_colors[j % len(seed_colors)])\n",
    "    for val, seed_nums in val_to_seeds.items():\n",
    "        dot_color = val_to_colors[val][0]\n",
    "        plt.scatter(val, i, color=dot_color, s=100, marker='o', zorder=5)\n",
    "        label_text = \",\".join(str(num) for num in seed_nums)\n",
    "        plt.text(val, i - 0.17, label_text, color=dot_color, ha='center', va='top', \n",
    "                 fontsize=10, fontweight='bold', zorder=6)\n",
    "plt.yticks(range(len(labels)), labels, fontsize=9)\n",
    "plt.xlabel(\"Success Rate\")\n",
    "plt.title(\"Task Success Rate (100 Demonstration Episodes)\")\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label=f'Seed {s}', markerfacecolor=seed_colors[i], markersize=10)\n",
    "    for i, s in enumerate(SEED_LIST)\n",
    "]\n",
    "plt.legend(handles=legend_elements + [Line2D([0], [0], color='skyblue', lw=10, label=\"Mean ± std\")], fontsize=10)\n",
    "plt.xlim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"best6_multiseed_success_rate_inline_seed_numbers.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [s['return_mean'] for s in sorted_summary]\n",
    "stds = [s['return_std'] for s in sorted_summary]\n",
    "plt.figure(figsize=(10, max(6, len(labels)*0.5)))\n",
    "plt.barh(range(len(means)), means, xerr=stds, color='salmon', alpha=0.7, label=\"Mean ± std\")\n",
    "for i, s in enumerate(sorted_summary):\n",
    "    val_to_seeds = {}\n",
    "    val_to_colors = {}\n",
    "    for j, val in enumerate(s['returns']):\n",
    "        if val not in val_to_seeds:\n",
    "            val_to_seeds[val] = []\n",
    "            val_to_colors[val] = []\n",
    "        val_to_seeds[val].append(SEED_LIST[j])\n",
    "        val_to_colors[val].append(seed_colors[j % len(seed_colors)])\n",
    "    for val, seed_nums in val_to_seeds.items():\n",
    "        dot_color = val_to_colors[val][0]\n",
    "        plt.scatter(val, i, color=dot_color, s=100, marker='o', zorder=5)\n",
    "        label_text = \",\".join(str(num) for num in seed_nums)\n",
    "        plt.text(val, i - 0.17, label_text, color=dot_color, ha='center', va='top', \n",
    "                 fontsize=10, fontweight='bold', zorder=6)\n",
    "plt.yticks(range(len(labels)), labels, fontsize=9)\n",
    "plt.xlabel(\"Episode Return\")\n",
    "plt.title(\"Episode Return (100 Demonstration Episodes)\")\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label=f'Seed {s}', markerfacecolor=seed_colors[i], markersize=10)\n",
    "    for i, s in enumerate(SEED_LIST)\n",
    "]\n",
    "plt.legend(handles=legend_elements + [Line2D([0], [0], color='salmon', lw=10, label=\"Mean ± std\")], fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"best6_multiseed_episode_return_inline_seed_numbers.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
