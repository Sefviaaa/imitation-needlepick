{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd31c49",
   "metadata": {},
   "source": [
    "# BC MLP Policy: Training, Evaluation, and Plotting (2000 Episodes)\n",
    "This notebook combines training, evaluation, and plotting for MLP models on the NeedlePick task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from surrol.tasks.needle_pick import NeedlePick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f64167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set random seeds for reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ca900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_obs(obs):\n",
    "    return np.concatenate([obs['observation'], obs['achieved_goal'], obs['desired_goal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Policy network (MLP) ---\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(128,128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_dim = h\n",
    "        layers.append(nn.Linear(last_dim, act_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training function ---\n",
    "def train_mlp(obs_data, act_data, epochs=50, batch_size=128, lr=1e-3, hidden_sizes=(128,128), val_split=0.1):\n",
    "    obs_dim = obs_data.shape[1]\n",
    "    act_dim = act_data.shape[1]\n",
    "    policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=hidden_sizes).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    obs_data = np.array(obs_data)\n",
    "    act_data = np.array(act_data)\n",
    "    n_val = int(len(obs_data) * val_split)\n",
    "    if n_val > 0:\n",
    "        val_obs = torch.tensor(obs_data[:n_val], dtype=torch.float32).to(device)\n",
    "        val_act = torch.tensor(act_data[:n_val], dtype=torch.float32).to(device)\n",
    "        train_obs = torch.tensor(obs_data[n_val:], dtype=torch.float32).to(device)\n",
    "        train_act = torch.tensor(act_data[n_val:], dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        train_obs = torch.tensor(obs_data, dtype=torch.float32).to(device)\n",
    "        train_act = torch.tensor(act_data, dtype=torch.float32).to(device)\n",
    "        val_obs = val_act = None\n",
    "    num_samples = train_obs.shape[0]\n",
    "    losses = []\n",
    "    val_mse = []\n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        epoch_loss = 0\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        obs_shuffled = train_obs[indices]\n",
    "        act_shuffled = train_act[indices]\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            obs_batch = obs_shuffled[start:end]\n",
    "            act_batch = act_shuffled[start:end]\n",
    "            optimizer.zero_grad()\n",
    "            act_pred = policy(obs_batch)\n",
    "            loss = loss_fn(act_pred, act_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(obs_batch)\n",
    "        epoch_loss /= num_samples\n",
    "        losses.append(epoch_loss)\n",
    "        if val_obs is not None:\n",
    "            policy.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = policy(val_obs)\n",
    "                val_loss = loss_fn(val_pred, val_act).item()\n",
    "                val_mse.append(val_loss)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == epochs:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\" +\n",
    "                  (f\", Val: {val_mse[-1]:.6f}\" if val_mse else \"\"))\n",
    "    return policy, losses, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, episodes=10, max_steps=200):\n",
    "    env = NeedlePick(render_mode=None)\n",
    "    success_count = 0\n",
    "    returns = []\n",
    "    for ep in range(episodes):\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        for step in range(max_steps):\n",
    "            obs_in = np.concatenate([\n",
    "                obs['observation'],\n",
    "                obs['achieved_goal'],\n",
    "                obs['desired_goal']\n",
    "            ])\n",
    "            inp = torch.tensor(obs_in, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                action = policy(inp).cpu().numpy()[0]\n",
    "            if hasattr(env, 'action_space'):\n",
    "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info.get('is_success', False):\n",
    "                success_count += 1\n",
    "                break\n",
    "            if done:\n",
    "                break\n",
    "        returns.append(total_reward)\n",
    "    avg_return = np.mean(returns)\n",
    "    success_rate = success_count / episodes\n",
    "    try:\n",
    "        env.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Exception during env.close(): {e}\")\n",
    "    del env\n",
    "    return avg_return, success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5c91e",
   "metadata": {},
   "source": [
    "# 1. Training and Evaluation : Hyperparameter Grid Search\n",
    "Train MLP policies with different hyperparameters and save models/metrics. Then, evaluate each trained policy and save results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main workflow ---\n",
    "out_dir = \"mlp_bc_models\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Load expert data\n",
    "with open(\"expert_trajectories.pkl\", \"rb\") as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "# Prepare dataset\n",
    "observations = []\n",
    "actions = []\n",
    "for episode in trajectories:\n",
    "    obs_list = episode['observations']\n",
    "    act_list = episode['actions']\n",
    "    for obs, act in zip(obs_list, act_list):\n",
    "        obs_in = concat_obs(obs)\n",
    "        observations.append(obs_in)\n",
    "        actions.append(act)\n",
    "observations = np.array(observations)\n",
    "actions = np.array(actions)\n",
    "print(f\"Collected {len(observations)} expert steps.\")\n",
    "\n",
    "obs_dim = observations.shape[1]\n",
    "act_dim = actions.shape[1]\n",
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-3, 3e-4, 1e-4]\n",
    "hidden_sizes_list = [(128,128), (256,256)]\n",
    "batch_sizes = [64, 128]\n",
    "epochs_list = [75, 150]\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Grid search: Train and Evaluate\n",
    "for lr in learning_rates:\n",
    "    for hidden_sizes in hidden_sizes_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                print(\"\\n========================================\")\n",
    "                print(f\"Training MLP: lr={lr}, hidden={hidden_sizes}, batch={batch_size}, epochs={epochs}\")\n",
    "                policy, losses, val_mse = train_mlp(\n",
    "                    observations, actions, epochs=epochs, batch_size=batch_size,\n",
    "                    lr=lr, hidden_sizes=hidden_sizes\n",
    "                )\n",
    "                # Save model and metrics\n",
    "                model_name = f\"mlp_bc_lr{lr}_hid{hidden_sizes[0]}_{hidden_sizes[1]}_bs{batch_size}_ep{epochs}\"\n",
    "                torch.save(policy.state_dict(), os.path.join(out_dir, f\"{model_name}.pth\"))\n",
    "                np.save(os.path.join(out_dir, f\"{model_name}_train_losses.npy\"), np.array(losses))\n",
    "                np.save(os.path.join(out_dir, f\"{model_name}_val_mse.npy\"), np.array(val_mse))\n",
    "                # Evaluate\n",
    "                avg_return, success_rate = evaluate_policy(policy, episodes=10, max_steps=200)\n",
    "                np.save(os.path.join(out_dir, f\"{model_name}_eval_success_rate.npy\"), np.array([success_rate]))\n",
    "                np.save(os.path.join(out_dir, f\"{model_name}_eval_return.npy\"), np.array([avg_return]))\n",
    "                print(f\"Saved model and logs to {out_dir}: {model_name}\")\n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'lr': lr,\n",
    "                    'hidden_sizes': hidden_sizes,\n",
    "                    'batch_size': batch_size,\n",
    "                    'epochs': epochs,\n",
    "                    'final_val_mse': val_mse[-1] if val_mse else None,\n",
    "                    'avg_return': avg_return,\n",
    "                    'success_rate': success_rate\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "csv_path = os.path.join(out_dir, \"evaluation_results_mlp.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"model_name\", \"lr\", \"hidden_sizes\", \"batch_size\", \"epochs\", \"final_val_mse\", \"avg_return\", \"success_rate\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "print(\"\\n===== MLP Hyperparameter Comparison Results =====\")\n",
    "for res in sorted(results, key=lambda x: (-(x['success_rate'] or 0), -(x['avg_return'] or 0))):\n",
    "    print(f\"{res['model_name']}: Success={res['success_rate']*100:.1f}%, Return={res['avg_return']:.2f}, Final Val MSE={res['final_val_mse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b51775",
   "metadata": {},
   "source": [
    "# 2. Plotting: Visualize Training and Evaluation Metrics\n",
    "Plot training loss, validation MSE, success rate, and episode return for all hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfe852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plotting ----\n",
    "# Collect results for plotting\n",
    "models = [r['model_name'] for r in results]\n",
    "successes = [r['success_rate'] for r in results]\n",
    "returns = [r['avg_return'] for r in results]\n",
    "\n",
    "# Find the best configs for each metric\n",
    "best_val = min(results, key=lambda r: r['final_val_mse'] if r['final_val_mse'] is not None else float('inf'))\n",
    "best_success = max(results, key=lambda r: r['success_rate'])\n",
    "best_return = max(results, key=lambda r: r['avg_return'])\n",
    "\n",
    "# 1. Training Loss (MSE) for best configs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.load(os.path.join(out_dir, f\"{best_val['model_name']}_train_losses.npy\")), label=f\"Best Val MSE ({best_val['model_name']})\")\n",
    "plt.plot(np.load(os.path.join(out_dir, f\"{best_success['model_name']}_train_losses.npy\")), label=f\"Best Success ({best_success['model_name']})\")\n",
    "plt.plot(np.load(os.path.join(out_dir, f\"{best_return['model_name']}_train_losses.npy\")), label=f\"Best Return ({best_return['model_name']})\")\n",
    "plt.title(\"Training Loss (MSE) (2000 Episode) for Best Configs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"mlp_hyperparam_training_loss.png\"))\n",
    "plt.show()\n",
    "\n",
    "# 2. Validation MSE for best configs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.load(os.path.join(out_dir, f\"{best_val['model_name']}_val_mse.npy\")), label=f\"Best Val MSE ({best_val['model_name']})\")\n",
    "plt.plot(np.load(os.path.join(out_dir, f\"{best_success['model_name']}_val_mse.npy\")), label=f\"Best Success ({best_success['model_name']})\")\n",
    "plt.plot(np.load(os.path.join(out_dir, f\"{best_return['model_name']}_val_mse.npy\")), label=f\"Best Return ({best_return['model_name']})\")\n",
    "plt.title(\"Validation MSE (2000 Episode) for Best Configs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"mlp_hyperparam_val_mse.png\"))\n",
    "plt.show()\n",
    "\n",
    "# 3. Task Success Rate (All Hyperparams)\n",
    "success_sorted = sorted(zip(successes, models), reverse=True)\n",
    "success_vals, success_labels = zip(*success_sorted)\n",
    "plt.figure(figsize=(10, max(6, len(models)*0.3)))\n",
    "plt.barh(range(len(success_vals)), success_vals, color='skyblue')\n",
    "plt.yticks(range(len(success_labels)), success_labels, fontsize=7)\n",
    "plt.xlabel(\"Success Rate\")\n",
    "plt.title(\"Task Success Rate (All Hyperparams) (2000 Episode)\")\n",
    "plt.xlim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"mlp_success_rate_2000.png\"))\n",
    "plt.show()\n",
    "\n",
    "# 4. Episode Return (All Hyperparams)\n",
    "return_sorted = sorted(zip(returns, models), reverse=True)\n",
    "return_vals, return_labels = zip(*return_sorted)\n",
    "plt.figure(figsize=(10, max(6, len(models)*0.3)))\n",
    "plt.barh(range(len(return_vals)), return_vals, color='salmon')\n",
    "plt.yticks(range(len(return_labels)), return_labels, fontsize=7)\n",
    "plt.xlabel(\"Episode Return\")\n",
    "plt.title(\"Episode Return (All Hyperparams) (2000 Episode)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"mlp_episode_return_2000.png\"))\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Plotted and saved in {out_dir}:\\n\"\n",
    "    \" - Training loss: mlp_hyperparam_training_loss.png\\n\"\n",
    "    \" - Validation MSE: mlp_hyperparam_val_mse.png\\n\"\n",
    "    \" - Success rate: mlp_success_rate_2000.png\\n\"\n",
    "    \" - Episode return: mlp_episode_return_2000.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
